{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"figure.figsize\": (12,9),\n",
    "          \"font.size\": 20,\n",
    "          \"font.weight\": \"normal\",\n",
    "          \"xtick.major.size\": 9,\n",
    "          \"xtick.minor.size\": 4,\n",
    "          \"ytick.major.size\": 9,\n",
    "          \"ytick.minor.size\": 4,\n",
    "          \"xtick.major.width\": 4,\n",
    "          \"xtick.minor.width\": 3,\n",
    "          \"ytick.major.width\": 4,\n",
    "          \"ytick.minor.width\": 3,\n",
    "          \"xtick.major.pad\": 8,\n",
    "          \"xtick.minor.pad\": 8,\n",
    "          \"ytick.major.pad\": 8,\n",
    "          \"ytick.minor.pad\": 8,\n",
    "          \"lines.linewidth\": 3,\n",
    "          \"lines.markersize\": 10,\n",
    "          \"axes.linewidth\": 4,\n",
    "          \"legend.loc\": \"best\",\n",
    "          \"text.usetex\": False,    \n",
    "          \"xtick.labelsize\" : 20,\n",
    "          \"ytick.labelsize\" : 20,\n",
    "          }\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astroalign as aa\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "from astropy import units as un\n",
    "from astropy.coordinates import SkyCoord\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as spstats\n",
    "\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from astropy.io import fits\n",
    "\n",
    "from astroquery.vizier import Vizier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformation(target_coords, field_coords, min_sep=5./60./60., print_outs=False):\n",
    "    '''\n",
    "    Find the transformation matrix to adjust your field coordinates to\n",
    "    match the target coordinates.\n",
    "    \n",
    "    This is a slightly different version for the MC investigation\n",
    "    of the uncertainties.\n",
    "    \n",
    "    Here we have some target coordinates, which are the coordinates\n",
    "    of the sources that you know have good astrometry. Then you have\n",
    "    the coordinates of every source in your field. These are the\n",
    "    coordinates that you want to transform such that they match\n",
    "    the target coordinates. This code finds the transformation\n",
    "    matrix to shift all of your field coordinates to match\n",
    "    the target coordinates.\n",
    "    \n",
    "    Args:\n",
    "    target_coords (SkyCoord): a SkyCoord object with the coordinates\n",
    "                              of the sources that have known\n",
    "                              astrometric accuracy\n",
    "    field_coords (SkyCoord): a SkyCoord object with the coordinates\n",
    "                             of the sources that do not have\n",
    "                             known astrometric accuracy\n",
    "    kwargs:\n",
    "    min_sep (float): the minimum separation (in degrees)\n",
    "                     for a match between your target source\n",
    "                     coordinates and a source in your\n",
    "                     field coordinates\n",
    "                     Default: 5./60./60. degrees\n",
    "    print_outs (bool): if you want to print out some\n",
    "                       information about the matches\n",
    "                       as you go. True means the info\n",
    "                       will be printed.\n",
    "                       Default: False\n",
    "    Returns:\n",
    "    (transform, [np.nanmean(original_separations),\n",
    "                np.nanmean(new_separations),\n",
    "                np.nanmedian(original_separations),\n",
    "                np.nanmedian(new_separations),\n",
    "                np.nanmax(original_separations),\n",
    "                np.nanmax(new_separations)],\n",
    "    transform(field))\n",
    "\n",
    "    transform: this is the function to transform your\n",
    "               field_coords to match the target_coords\n",
    "    original_separations is the separation between your\n",
    "    target sources and field sources before transformation.\n",
    "    new_separations is the separation between your\n",
    "    target sources and field sources after transformation.\n",
    "    transform (field): gives the transformed coordinates\n",
    "                       of the reference MeerKAT sources\n",
    "    '''    \n",
    "    matches = []\n",
    "    # find which sources in your field_sources\n",
    "    # match the target_coords\n",
    "    for t, tg in enumerate(target_coords):\n",
    "        seps = tg.separation(field_coords)\n",
    "        if np.nanmin(seps.deg) < min_sep:\n",
    "            matches.append([tg.ra.deg, tg.dec.deg,\n",
    "                            field_coords[np.nanargmin(seps.deg)].ra.deg,\n",
    "                            field_coords[np.nanargmin(seps.deg)].dec.deg,\n",
    "                            np.nanmin(seps.deg)*60.*60.])\n",
    "\n",
    "    matches = np.array(matches)\n",
    "    target = matches[:, :2]\n",
    "    field = matches[:, 2:4]\n",
    "\n",
    "    n = field.shape[0]\n",
    "    # Small functions to pad arrays withs ones\n",
    "    pad = lambda x: np.hstack([x, np.ones((x.shape[0], 1))])\n",
    "    unpad = lambda x: x[:,:-1]\n",
    "\n",
    "    # Pad your coordinates\n",
    "    X = pad(field)\n",
    "    Y = pad(target)\n",
    "\n",
    "    # Solve the linear least squares\n",
    "    # problem of transforming the X coords\n",
    "    # into the Y coords\n",
    "    A, res, rank, s = np.linalg.lstsq(X, Y)\n",
    "\n",
    "    # Make the function to transform\n",
    "    # your X coords to match the\n",
    "    # Y coord frame\n",
    "    transform = lambda x: unpad(np.dot(pad(x), A))\n",
    "\n",
    "    # Work out whether the field sources have\n",
    "    # actually moved closer to the target\n",
    "    # sources after transformation\n",
    "    field_sc = SkyCoord(field, unit=(un.deg, un.deg))\n",
    "    field_sc_T = SkyCoord(transform(field), unit=(un.deg, un.deg))\n",
    "    target_sc = SkyCoord(target, unit=(un.deg, un.deg))\n",
    "    original_separations = []\n",
    "    new_separations = []\n",
    "    for m, mkt in enumerate(field_sc):\n",
    "        sep0 = mkt.separation(target_sc[m])\n",
    "        original_separations.append(sep0.deg*60.*60.)\n",
    "        sep1 = field_sc_T[m].separation(target_sc[m])\n",
    "        new_separations.append(sep1.deg*60.*60.)\n",
    "    original_separations = np.array(original_separations)\n",
    "    new_separations = np.array(new_separations)\n",
    "    \n",
    "    if print_outs:\n",
    "        # Print some useful, quick look values]\n",
    "        print(('Separation between the '\n",
    "               'ATPMN reference souces '\n",
    "               'and MeerKAT reference\\n'\n",
    "               'sources in arcsec before '\n",
    "               '(left column) and after '\n",
    "               '(right column) transformation.'))\n",
    "        print(np.hstack((matches,\n",
    "                         np.expand_dims(new_separations,\n",
    "                                        axis=1)))[:, 4:])    \n",
    "        print(('Original mean sep: {0:.4f}\", '\n",
    "               'new mean sep: {1:.4f}\"').format(np.nanmean(original_separations),\n",
    "                                                np.nanmean(new_separations)))\n",
    "        print(('Original median sep: {0:.4f}\", '\n",
    "               'new median sep: {1:.4f}\"').format(np.nanmedian(original_separations),\n",
    "                                                  np.nanmedian(new_separations)))\n",
    "    \n",
    "    return (transform, [np.nanmean(original_separations),\n",
    "                        np.nanmean(new_separations),\n",
    "                        np.nanmedian(original_separations),\n",
    "                        np.nanmedian(new_separations),\n",
    "                        np.nanmax(original_separations),\n",
    "                        np.nanmax(new_separations)],\n",
    "            transform(field))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo uncertainties\n",
    "\n",
    "We want to determine the dominant source of uncertainty for the astrometry of the sources in the GX 339-4 field. We know that the median astrometric uncertainty for the ATPMN sources in 0.4 arcsec, and we have 6 ATPMN sources that match MeerKAT sources in the GX 339-4 field. So we are going to scatter the ATPMN sources usin an uncertainty of 0.4 arcsec and see how that affects the position and scatter of the MeerKAT sources after transforming them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the ATPMN sources that are in the GX 339 FoV\n",
    "# using Vizier\n",
    "result_atpmn = Vizier.query_region(SkyCoord(ra=255.706, dec=-48.790,\n",
    "                                            unit=(un.deg, un.deg),\n",
    "                                            frame='icrs'),\n",
    "                             width=\"180m\",\n",
    "                             catalog=[\"J/MNRAS/422/1527/atpmncat\"])\n",
    "\n",
    "# Remove the ATPMN sources that match extended sources\n",
    "# in the GX 339-4 field, as these won't have accurate\n",
    "# positions\n",
    "remove_sources = SkyCoord(np.array([254.57271, 254.56767,\n",
    "                                    254.41062, 257.15887])*un.deg,\n",
    "                          np.array([-47.51383, -47.51764,\n",
    "                                    -50.19133, -49.00014])*un.deg)\n",
    "ras = []\n",
    "decs = []\n",
    "for ra in result_atpmn[0]['RAJ2000']:\n",
    "    ras.append(ra)\n",
    "for dec in result_atpmn[0]['DEJ2000']:\n",
    "    decs.append(dec)\n",
    "\n",
    "# Get the ATPMN coordinates of the sources\n",
    "# that you're going to use for astrometry\n",
    "og_atpmn_coords = SkyCoord(ras, decs, unit=(un.hourangle, un.deg))\n",
    "atpmn_coords = []\n",
    "for a, atpmn in enumerate(og_atpmn_coords):\n",
    "    r_seps = atpmn.separation(remove_sources)\n",
    "    if np.nanmin(r_seps.deg) > 10./60./60.:\n",
    "        atpmn_coords.append([atpmn.ra.deg, atpmn.dec.deg])\n",
    "# Add the GX 339 and pulsar coordinates\n",
    "# to your astrometry coordinates\n",
    "astro_coords = np.array(atpmn_coords)\n",
    "\n",
    "# Print the coordinates in DS9 region file\n",
    "# format so you can show them on the GX 339-4\n",
    "# images\n",
    "astro_coords = SkyCoord(np.array(astro_coords), unit=(un.deg, un.deg))\n",
    "for a, astro in enumerate(astro_coords):\n",
    "    text = ('circle({0:.5f},{1:.5f},5.000\") '\n",
    "            '# color=magenta '\n",
    "            'text={{{2}}}').format(astro.ra.deg,\n",
    "                                   astro.dec.deg, a)\n",
    "    print(text)\n",
    "    \n",
    "    text = ('circle({0:.5f},{1:.5f},70.000\") '\n",
    "            '# color=cyan').format(astro.ra.deg,\n",
    "                                   astro.dec.deg)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 5000\n",
    "num_sources = np.shape(astro_coords)[0]\n",
    "min_sep = 10./60./60.\n",
    "\n",
    "# The pulsar and GX339 coordinates\n",
    "# in case you need them\n",
    "psr_coords = SkyCoord(np.array([['17h03m54.53', '-48d52m01.0']]))\n",
    "gx_psr_coords = np.array([[psr_coords.ra.deg[0], psr_coords.dec.deg[0]],\n",
    "                          [255.70575354295, -48.78976826464]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_array_ras = np.ones((num_sources, num_iter))\n",
    "random_array_decs = np.ones((num_sources, num_iter))\n",
    "\n",
    "for r, row in enumerate(astro_coords):\n",
    "    random_array_ras[r] = np.random.normal(loc=row.ra.deg, scale=0.4/60./60., size=num_iter)\n",
    "    random_array_decs[r] = np.random.normal(loc=row.dec.deg, scale=0.4/60./60., size=num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_target_ras = np.ones((num_sources, num_iter))\n",
    "random_target_decs = np.ones((num_sources, num_iter))\n",
    "\n",
    "target_srlf = ('/raid/driessen/FlareStars/'\n",
    "               'GX339/DeepImages/'\n",
    "               'gx_deep_CORRCOL_DDE.app.restored_edited.pybdsm.srl')\n",
    "target_srl = pd.read_csv(target_srlf, header=4)\n",
    "target_srl.columns = target_srl.columns.str.replace(' ', '')\n",
    "target_coordinates = SkyCoord(target_srl['RA']*un.deg,\n",
    "                              target_srl['DEC']*un.deg)\n",
    "target_raw_coordinates = np.vstack((target_srl['RA'], target_srl['DEC'])).T\n",
    "\n",
    "for c in range(len(random_array_ras[0])):\n",
    "    astro_coords_array = np.ones((num_sources, 2))\n",
    "    \n",
    "    astro_coords_array[:, 0] = random_array_ras[:, c]\n",
    "    astro_coords_array[:, 1] = random_array_decs[:, c]\n",
    "    \n",
    "    astro_coords_array_sc = SkyCoord(np.array(astro_coords_array), unit=(un.deg, un.deg))\n",
    "    \n",
    "    A_tf, av_seps, target_new_coords = get_transformation(astro_coords_array_sc,\n",
    "                                       target_coordinates,\n",
    "                                       min_sep=min_sep)\n",
    "    \n",
    "    target_transformed = A_tf(target_raw_coordinates)\n",
    "    \n",
    "    random_target_ras[:, c] = target_new_coords[:, 0]\n",
    "    random_target_decs[:, c] = target_new_coords[:, 1]\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r, row in enumerate(random_target_ras):\n",
    "    print('RA mean: {0:.6f} std: {1:3f} asec'.format(np.mean(row),\n",
    "                                              np.std(row)*60.*60.))\n",
    "    print('Dec mean: {0:.6f} std {1:3f} asec'.format(np.mean(random_target_decs[r]),\n",
    "                                              np.std(random_target_decs[r])*60.*60.))\n",
    "    print('--------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.4 arcsec still dominates the uncertainties, so we are going to use 0.4 arcsec as our uncertainty in RA and DEC for our interesting sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we take the coordinates of every source in the deep stack GX 339-4 image, calculate the transformation, and apply the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and apply the transformations\n",
    "# for just the deep GX 339-4 image\n",
    "\n",
    "target_srlf = ('/raid/driessen/FlareStars/'\n",
    "               'GX339/DeepImages/'\n",
    "               'gx_deep_CORRCOL_DDE.app.restored_edited.pybdsm.srl')\n",
    "target_srl = pd.read_csv(target_srlf, header=4)\n",
    "target_srl.columns = target_srl.columns.str.replace(' ', '')\n",
    "target_coordinates = SkyCoord(target_srl['RA']*un.deg,\n",
    "                              target_srl['DEC']*un.deg)\n",
    "\n",
    "average_separations = []\n",
    "for s, field_srlf in enumerate([target_srlf]):\n",
    "    bn = field_srlf.split('.srl')[0]\n",
    "    bn = bn.split('/')[-1]\n",
    "    \n",
    "    new_srlf_name = ('/raid/driessen/FlareStars/'\n",
    "                     'GX339/MFS_images/'\n",
    "                     '{0}_tf.srl').format(bn)\n",
    "    \n",
    "    field_srl = pd.read_csv(field_srlf, header=4)\n",
    "    field_srl.columns = field_srl.columns.str.replace(' ', '')\n",
    "    try:\n",
    "        field_coordinates = SkyCoord(field_srl['RA']*un.deg,\n",
    "                                     field_srl['DEC']*un.deg)\n",
    "        field_raw_coordinates = np.vstack((field_srl['RA'],\n",
    "                                           field_srl['DEC'])).T\n",
    "\n",
    "        (A_tf,\n",
    "         av_seps,\n",
    "         match_coords) = get_transformation(astro_coords,\n",
    "                                            field_coordinates,\n",
    "                                            min_sep=2./60./60.,\n",
    "                                            print_outs=True)\n",
    "        \n",
    "        average_separations.append(av_seps)\n",
    "\n",
    "        field_transformed = A_tf(field_raw_coordinates)\n",
    "\n",
    "        field_srl['RA_tf'] = field_transformed[:, 0]\n",
    "        field_srl['DEC_tf'] = field_transformed[:, 1]\n",
    "        field_srl.to_csv(new_srlf_name, index=False)\n",
    "    except KeyError:\n",
    "        print('***********************************')\n",
    "        print(field_srlf)\n",
    "        print('***********************************')\n",
    "    \n",
    "    print('-------------------------------')\n",
    "    print('-------------------------------')\n",
    "average_separations = np.array(average_separations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get out the coordinates of our sources of interest, and double check that the 0.4 arcsec still dominates the uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the updated coordinate file of the deep\n",
    "# GX 339-4 image\n",
    "deep_srl = pd.read_csv(('/raid/driessen/FlareStars/'\n",
    "                        'GX339/MFS_images/'\n",
    "                        'gx_deep_CORRCOL_DDE.app.'\n",
    "                        'restored_edited.pybdsm_tf.srl'))\n",
    "deep_srl_coords = SkyCoord(deep_srl['RA_tf']*un.deg, deep_srl['DEC_tf']*un.deg)\n",
    "\n",
    "print(('Mean RA uncertainty for all sources: '\n",
    "       '{:.4f}asec').format(np.mean(deep_srl['E_RA']*60.*60.)))\n",
    "print(('Mean Dec uncertainty for all sources: '\n",
    "       '{:.4f}asec').format(np.mean(deep_srl['E_DEC']*60.*60.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read your old coordinates for the interesting\n",
    "# sources\n",
    "long_is = pd.read_csv(('/raid/driessen/FlareStars/'\n",
    "                       'GX339/LightCurves_SN3/'\n",
    "                       '2021.02.02_LongVariableSources.csv'))\n",
    "long_is_coords = SkyCoord(long_is['ra_deg']*un.deg,\n",
    "                          long_is['dec_deg']*un.deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the transformed, deep image coordinates\n",
    "# to the information about each long-term\n",
    "# variable source\n",
    "\n",
    "new_coord_info = []\n",
    "\n",
    "for l, lis in enumerate(long_is_coords):\n",
    "    seps = lis.separation(deep_srl_coords)\n",
    "    \n",
    "    if np.nanmin(seps.deg) < 2./60./60.:\n",
    "        print('{0:.5f} {1:.5f}'.format(lis.ra.deg,\n",
    "                                       lis.dec.deg))\n",
    "        print('{0:.5f} {1:.5f}'.format(deep_srl_coords[np.nanargmin(seps.deg)].ra.deg,\n",
    "                                       deep_srl_coords[np.nanargmin(seps.deg)].dec.deg))\n",
    "        print(('Separation between old '\n",
    "               'and new coordinates: '\n",
    "               '{:.6f} arcsec').format(np.nanmin(seps.deg)*60.*60.))\n",
    "        new_coord_info.append([deep_srl.iloc[np.nanargmin(seps.deg)]['RA_tf'],\n",
    "                               deep_srl.iloc[np.nanargmin(seps.deg)]['E_RA'],\n",
    "                               deep_srl.iloc[np.nanargmin(seps.deg)]['DEC_tf'],\n",
    "                               deep_srl.iloc[np.nanargmin(seps.deg)]['E_DEC']])\n",
    "        print('-----------------------------')\n",
    "new_coord_info = np.array(new_coord_info)\n",
    "\n",
    "# Add the new information to the DataFrame\n",
    "# and save it\n",
    "long_is['RA_tf_deg'] = new_coord_info[:, 0]\n",
    "long_is['E_RA_deg'] = new_coord_info[:, 1]\n",
    "long_is['DEC_tf_deg'] = new_coord_info[:, 2]\n",
    "long_is['E_DEC_deg'] = new_coord_info[:, 3]\n",
    "\n",
    "long_is['E_RA_tf_deg'] = 0.4/60./60.\n",
    "long_is['E_DEC_tf_deg'] = 0.4/60./60.\n",
    "\n",
    "long_is.to_csv(('/raid/driessen/'\n",
    "                'FlareStars/GX339/'\n",
    "                'MFS_images/'\n",
    "                '2021.02.11_'\n",
    "                'LongVariableSources.csv'),\n",
    "               index=False)\n",
    "\n",
    "print('*****************')\n",
    "print('Done')\n",
    "print('*****************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(('Mean RA uncertainty '\n",
    "       'for long variable '\n",
    "       'sources: {:.4f}asec').format(np.mean(long_is['E_RA_deg'])*60.*60))\n",
    "print(('Mean DEC uncertainty '\n",
    "       'for long variable '\n",
    "       'sources: {:.4f}asec').format(np.mean(long_is['E_DEC_deg'])*60.*60))\n",
    "print('----------------')\n",
    "print(('Max RA uncertainty '\n",
    "       'for long variable '\n",
    "       'sources: {:.4f}asec').format(np.max(long_is['E_RA_deg'])*60.*60))\n",
    "print(('Max DEC uncertainty '\n",
    "       'for long variable '\n",
    "       'sources: {:.4f}asec').format(np.max(long_is['E_DEC_deg'])*60.*60))\n",
    "print('----------------')\n",
    "print('----------------')\n",
    "print(('Mean RA uncertainty '\n",
    "       'for long variable '\n",
    "       'sources after shifting: {:.4f}asec').format(np.mean(long_is['E_RA_tf_deg'])*60.*60))\n",
    "print(('Mean DEC uncertainty '\n",
    "       'for long variable '\n",
    "       'sources after shifting: {:.4f}asec').format(np.mean(long_is['E_DEC_tf_deg'])*60.*60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that 0.4 arcsec is the dominant uncertainty, so we will use this to perform our source matches.\n",
    "\n",
    "Below is a demo of how we would calculate and apply the transformations to every epoch of the GX 339-4 ThunderKAT observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the transformations for each epoch of\n",
    "# the GX 339-4 observations and apply these.\n",
    "\n",
    "# All of the pyBDSF srl files\n",
    "srlfs = glob.glob(('/raid/driessen/'\n",
    "                   'FlareStars/GX339/'\n",
    "                   'LightCurves_SN3/'\n",
    "                   'SRLs/20*.srl'))\n",
    "\n",
    "average_separations = []\n",
    "for s, field_srlf in enumerate(srlfs):\n",
    "    # Set up the name for the file\n",
    "    # you'll save the position info\n",
    "    # as\n",
    "    bn = field_srlf.split('.srl')[0]\n",
    "    bn = bn.split('/')[-1]\n",
    "    new_srlf_name = ('/raid/driessen/FlareStars/'\n",
    "                     'GX339/MFS_images/'\n",
    "                     '{0}_tf.srl').format(bn)\n",
    "\n",
    "    # Read in the srl file\n",
    "    field_srl = pd.read_csv(field_srlf, header=4)\n",
    "    field_srl.columns = field_srl.columns.str.replace(' ', '')\n",
    "    try:\n",
    "        field_coordinates = SkyCoord(field_srl['RA']*un.deg,\n",
    "                                     field_srl['DEC']*un.deg)\n",
    "        field_raw_coordinates = np.vstack((field_srl['RA'], field_srl['DEC'])).T\n",
    "\n",
    "        # Get the transformation\n",
    "        (A_tf,\n",
    "         av_seps,\n",
    "         match_coords) = get_transformation(astro_coords,\n",
    "                                            field_coordinates,\n",
    "                                            min_sep=2./60./60.,\n",
    "                                            print_outs=False)\n",
    "        \n",
    "        average_separations.append(av_seps)\n",
    "\n",
    "        # Transform all of the sources in the srl file\n",
    "        field_transformed = A_tf(field_raw_coordinates)\n",
    "\n",
    "        # Save the transformed source coordinates\n",
    "        field_srl['RA_tf'] = field_transformed[:, 0]\n",
    "        field_srl['DEC_tf'] = field_transformed[:, 1]\n",
    "        field_srl.to_csv(new_srlf_name)\n",
    "    except KeyError:\n",
    "        print('***********************************')\n",
    "        print(field_srlf)\n",
    "        print('***********************************')\n",
    "    \n",
    "    print('-------------------------------')\n",
    "    print('-------------------------------')\n",
    "average_separations = np.array(average_separations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
